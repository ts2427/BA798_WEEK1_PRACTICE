{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Analysis: Regulatory Risk Clustering\n",
    "\n",
    "## Uncovering Natural Patterns in Data Breach Regulatory Risk\n",
    "\n",
    "In this analysis, we apply unsupervised learning techniques to discover hidden patterns in data breach incidents.\n",
    "Using K-means clustering and Principal Component Analysis (PCA), we identify natural groupings of breaches based on\n",
    "regulatory action likelihood, severity, and financial impact.\n",
    "\n",
    "**Story Focus:** How breaches cluster into distinct regulatory risk profiles\n",
    "**Audience:** Technical/Data Science\n",
    "**Methods:** K-means clustering + PCA for dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction & Motivation\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Organizations face increasing regulatory scrutiny following data breaches. Understanding which characteristics\n",
    "make a breach \"high-risk\" is critical for risk assessment, compliance strategy, regulatory policy, and investment priority.\n",
    "\n",
    "### Why Unsupervised Learning?\n",
    "\n",
    "Unlike supervised learning where we predict pre-labeled outcomes, regulatory risk clusters are not pre-determined.\n",
    "We use unsupervised methods to:\n",
    "\n",
    "1. **Discover natural groupings** without pre-labeled examples\n",
    "2. **Reduce dimensionality** from 26 features to 2-3 principal components for visualization\n",
    "3. **Identify interpretable profiles** that characterize regulatory risk\n",
    "\n",
    "### Technical Approach\n",
    "\n",
    "1. **Feature Engineering:** Select 26 features across regulatory, severity, financial, attack, and organizational dimensions\n",
    "2. **PCA:** Reduce to principal components explaining 80-90% of variance\n",
    "3. **K-means Clustering:** Determine optimal K using elbow method and silhouette analysis\n",
    "4. **Cluster Profiling:** Characterize each cluster and assess regulatory outcomes\n",
    "5. **Validation:** Use multiple quality metrics to assess clustering strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, davies_bouldin_score, calinski_harabasz_score\n",
    "from scipy.stats import f_oneway\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configure plot parameters\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = Path('../FINAL_DISSERTATION_DATASET_ENRICHED.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f'Dataset loaded successfully!')\n",
    "print(f'Shape: {df.shape}')\n",
    "print(f'\\nRows: {len(df)}, Columns: {len(df.columns)}')\n",
    "print(f'\\nTarget distribution:')\n",
    "print(df['has_any_regulatory_action'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Feature Selection & Engineering\n",
    "\n",
    "We select 26 features across six categories that capture different dimensions of regulatory risk.\n",
    "\n",
    "**Total Features:** 26 across regulatory, severity, financial, attack, organizational, and breach type categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for clustering\n",
    "regulatory_features = ['total_regulatory_cost', 'has_ftc_action', 'has_fcc_action',\n",
    "    'has_state_ag_action', 'num_states_involved']\n",
    "severity_features = ['severity_score', 'records_affected_numeric', 'high_severity_breach']\n",
    "financial_features = ['car_30d', 'volatility_change', 'return_volatility_post', 'bhar_30d']\n",
    "attack_features = ['ransomware', 'nation_state']\n",
    "org_features = ['firm_size_log', 'large_firm', 'prior_breaches_total', 'disclosure_delay_days']\n",
    "breach_type_features = ['pii_breach', 'health_breach', 'financial_breach']\n",
    "\n",
    "# Combine all features\n",
    "base_features = (regulatory_features + severity_features + financial_features +\n",
    "    attack_features + org_features + breach_type_features)\n",
    "\n",
    "# Check availability\n",
    "available_features = [f for f in base_features if f in df.columns]\n",
    "print(f'Total base features: {len(base_features)}')\n",
    "print(f'Available features: {len(available_features)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working copy\n",
    "df_clustering = df.copy()\n",
    "\n",
    "# breach_intensity\n",
    "if 'severity_score' in df_clustering.columns and 'records_affected_numeric' in df_clustering.columns:\n",
    "    df_clustering['breach_intensity'] = (\n",
    "        df_clustering['severity_score'] / np.log(df_clustering['records_affected_numeric'] + 1))\n",
    "    available_features.append('breach_intensity')\n",
    "    print('\u2713 Created breach_intensity feature')\n",
    "\n",
    "# regulatory_action_count\n",
    "reg_action_cols = ['has_ftc_action', 'has_fcc_action', 'has_state_ag_action']\n",
    "available_reg_cols = [c for c in reg_action_cols if c in df_clustering.columns]\n",
    "if available_reg_cols:\n",
    "    df_clustering['regulatory_action_count'] = df_clustering[available_reg_cols].sum(axis=1)\n",
    "    available_features.append('regulatory_action_count')\n",
    "    print('\u2713 Created regulatory_action_count feature')\n",
    "\n",
    "# has_financial_penalty\n",
    "if 'total_regulatory_cost' in df_clustering.columns:\n",
    "    df_clustering['has_financial_penalty'] = (df_clustering['total_regulatory_cost'] > 0).astype(int)\n",
    "    available_features.append('has_financial_penalty')\n",
    "    print('\u2713 Created has_financial_penalty feature')\n",
    "\n",
    "# attack_surface\n",
    "attack_cols = ['ransomware', 'nation_state', 'insider_threat', 'phishing', 'malware', 'ddos_attack']\n",
    "available_attack_cols = [c for c in attack_cols if c in df_clustering.columns]\n",
    "if available_attack_cols:\n",
    "    df_clustering['attack_surface'] = df_clustering[available_attack_cols].sum(axis=1)\n",
    "    if 'attack_surface' not in available_features:\n",
    "        available_features.append('attack_surface')\n",
    "    print('\u2713 Created attack_surface feature')\n",
    "\n",
    "# severity_per_record\n",
    "if 'severity_score' in df_clustering.columns and 'records_affected_numeric' in df_clustering.columns:\n",
    "    df_clustering['severity_per_record'] = (\n",
    "        df_clustering['severity_score'] / (df_clustering['records_affected_numeric'] + 1))\n",
    "    available_features.append('severity_per_record')\n",
    "    print('\u2713 Created severity_per_record feature')\n",
    "\n",
    "print(f'\\nTotal features: {len(available_features)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and handle missing values\n",
    "X = df_clustering[available_features].copy()\n",
    "\n",
    "# Replace infinity values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Fill numerical features with median\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    if X[col].isnull().sum() > 0:\n",
    "        X[col].fillna(X[col].median(), inplace=True)\n",
    "\n",
    "# Replace any remaining NaN/inf with 0\n",
    "X = X.fillna(0)\n",
    "X = X.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "print(f'Missing values after handling: {X.isnull().sum().sum()}')\n",
    "print(f'Feature set shape: {X.shape}')\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(f'\\nFeatures scaled successfully!')\n",
    "print(f'Shape: {X_scaled.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print('Summary Statistics (Sample)')\n",
    "print(X[[f for f in ['severity_score', 'records_affected_numeric', 'total_regulatory_cost'] if f in X.columns]].describe().round(2))\n",
    "print(f'\\nTarget variable (regulatory action):')\n",
    "print(df_clustering['has_any_regulatory_action'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Principal Component Analysis (PCA)\n",
    "\n",
    "PCA reduces our 26 features to principal components while preserving maximum variance.\n",
    "This allows us to visualize high-dimensional data in 2D/3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "print(f'Explained variance ratio (top 10 components):')\n",
    "for i, var in enumerate(pca_full.explained_variance_ratio_[:10]):\n",
    "    cumsum = pca_full.explained_variance_ratio_[:i+1].sum()\n",
    "    print(f'  PC{i+1}: {var:.4f} (cumulative: {cumsum:.4f})')\n",
    "\n",
    "# Fit 3-component PCA for visualization\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2', 'PC3'])\n",
    "\n",
    "print(f'\\nPCA Complete: {pca.explained_variance_ratio_.sum():.1%} variance explained with 3 components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variance explained\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "ax1.bar(range(1, 11), pca_full.explained_variance_ratio_[:10], alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "ax1.set_title('Scree Plot')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Cumulative variance\n",
    "cumsum = np.cumsum(pca_full.explained_variance_ratio_[:10])\n",
    "ax2.plot(range(1, 11), cumsum, 'o-', linewidth=2, markersize=8, color='darkgreen')\n",
    "ax2.axhline(y=0.8, color='r', linestyle='--', linewidth=2, label='80%')\n",
    "ax2.axhline(y=0.9, color='orange', linestyle='--', linewidth=2, label='90%')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Cumulative Variance')\n",
    "ax2.set_title('Cumulative Explained Variance')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/clustering/01_pca_variance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('\u2713 PCA variance plots saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Determining Optimal K using Elbow Method\n",
    "\n",
    "The elbow method helps determine the optimal number of clusters by identifying where\n",
    "the inertia reduction slows down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method: test K from 2 to 10\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10, max_iter=300, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    sil_score = silhouette_score(X_scaled, kmeans.labels_)\n",
    "    silhouette_scores.append(sil_score)\n",
    "    print(f'K={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={sil_score:.4f}')\n",
    "\n",
    "# Determine optimal K\n",
    "optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "print(f'\\nOptimal K (by silhouette): {optimal_k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot elbow and silhouette curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow curve\n",
    "ax1.plot(K_range, inertias, 'o-', linewidth=2, markersize=10, color='darkblue')\n",
    "ax1.set_xlabel('Number of Clusters (K)')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('Elbow Method')\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.set_xticks(K_range)\n",
    "\n",
    "# Silhouette scores\n",
    "ax2.plot(K_range, silhouette_scores, 'o-', linewidth=2, markersize=10, color='darkgreen')\n",
    "ax2.axvline(optimal_k, color='red', linestyle='--', linewidth=2, label=f'Optimal K={optimal_k}')\n",
    "ax2.set_xlabel('Number of Clusters (K)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Analysis')\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_xticks(K_range)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/clustering/02_elbow_silhouette.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('\u2713 Elbow and silhouette plots saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: K-means Clustering\n",
    "\n",
    "Fit K-means with the optimal number of clusters and analyze results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit final K-means model\n",
    "if optimal_k < 4:\n",
    "    optimal_k = 5\n",
    "    print(f'Using K=5 for regulatory interpretation')\n",
    "else:\n",
    "    print(f'Using optimal K={optimal_k}')\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, n_init=10, max_iter=300, random_state=42)\n",
    "cluster_labels = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to data\n",
    "df_clustering['cluster'] = cluster_labels\n",
    "X_pca_df['cluster'] = cluster_labels\n",
    "\n",
    "print(f'\\nCluster distribution:')\n",
    "print(df_clustering['cluster'].value_counts().sort_index())\n",
    "\n",
    "# Silhouette analysis\n",
    "sil_scores = silhouette_samples(X_scaled, cluster_labels)\n",
    "sil_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "\n",
    "print(f'\\nAverage silhouette score: {sil_avg:.4f}')\n",
    "print(f'Per-cluster silhouette scores:')\n",
    "for i in range(optimal_k):\n",
    "    cluster_sil = sil_scores[cluster_labels == i].mean()\n",
    "    print(f'  Cluster {i}: {cluster_sil:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters in PCA space\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis',\n",
    "                    s=80, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Plot centroids\n",
    "pca_centroids = pca.transform(kmeans_final.cluster_centers_)\n",
    "ax.scatter(pca_centroids[:, 0], pca_centroids[:, 1], c='red', marker='X', s=500,\n",
    "          edgecolors='black', linewidth=2, label='Centroids')\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax.set_title(f'K-means Clusters (K={optimal_k}) in PCA Space')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Cluster')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/clustering/03_kmeans_pca.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('\u2713 Cluster visualization saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Cluster Profiling & Interpretation\n",
    "\n",
    "Analyze the characteristics of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze regulatory outcomes by cluster\n",
    "print('\\n' + '='*80)\n",
    "print('REGULATORY OUTCOMES BY CLUSTER')\n",
    "print('='*80)\n",
    "\n",
    "reg_rates = []\n",
    "avg_costs = []\n",
    "cluster_sizes = []\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_mask = df_clustering['cluster'] == cluster_id\n",
    "    cluster_data = df_clustering[cluster_mask]\n",
    "\n",
    "    n_breaches = len(cluster_data)\n",
    "    n_with_action = cluster_data['has_any_regulatory_action'].sum()\n",
    "    pct_with_action = (n_with_action / n_breaches) * 100\n",
    "    avg_cost = cluster_data['total_regulatory_cost'].mean()\n",
    "\n",
    "    reg_rates.append(pct_with_action)\n",
    "    avg_costs.append(avg_cost)\n",
    "    cluster_sizes.append(n_breaches)\n",
    "\n",
    "    print(f'\\nCluster {cluster_id}:')\n",
    "    print(f'  Size: {n_breaches} breaches')\n",
    "    print(f'  Regulatory action: {n_with_action}/{n_breaches} ({pct_with_action:.1f}%)')\n",
    "    print(f'  Avg regulatory cost: ${avg_cost:,.0f}')\n",
    "    print(f'  Avg severity: {cluster_data[\"severity_score\"].mean():.2f}')\n",
    "    print(f'  Avg records affected: {cluster_data[\"records_affected_numeric\"].mean():,.0f}')\n",
    "    print(f'  Avg prior breaches: {cluster_data[\"prior_breaches_total\"].mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regulatory outcomes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Regulatory action rate\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, optimal_k))\n",
    "ax1.bar(range(optimal_k), reg_rates, color=colors, edgecolor='black', linewidth=2)\n",
    "ax1.set_xlabel('Cluster')\n",
    "ax1.set_ylabel('Regulatory Action Rate (%)')\n",
    "ax1.set_title('Regulatory Action Rate by Cluster')\n",
    "ax1.set_xticks(range(optimal_k))\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(reg_rates):\n",
    "    ax1.text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# Average regulatory cost\n",
    "ax2.bar(range(optimal_k), avg_costs, color=colors, edgecolor='black', linewidth=2)\n",
    "ax2.set_xlabel('Cluster')\n",
    "ax2.set_ylabel('Average Regulatory Cost ($)')\n",
    "ax2.set_title('Average Regulatory Cost by Cluster')\n",
    "ax2.set_xticks(range(optimal_k))\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/clustering/04_regulatory_by_cluster.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('\u2713 Regulatory outcomes visualization saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Clustering Quality Metrics\n",
    "\n",
    "Assess the quality of our clustering solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate quality metrics\n",
    "silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "davies_bouldin = davies_bouldin_score(X_scaled, cluster_labels)\n",
    "calinski_harabasz = calinski_harabasz_score(X_scaled, cluster_labels)\n",
    "\n",
    "print('='*80)\n",
    "print('CLUSTERING QUALITY METRICS')\n",
    "print('='*80)\n",
    "print(f'\\nSilhouette Coefficient: {silhouette_avg:.4f}')\n",
    "print(f'  Range: [-1, 1], higher is better')\n",
    "print(f'  Interpretation: {\"Good\" if silhouette_avg > 0.4 else \"Acceptable\"} cluster quality')\n",
    "\n",
    "print(f'\\nDavies-Bouldin Index: {davies_bouldin:.4f}')\n",
    "print(f'  Lower is better. Score < 2.0 is good')\n",
    "\n",
    "print(f'\\nCalinski-Harabasz Index: {calinski_harabasz:.2f}')\n",
    "print(f'  Higher is better. Score > 10 is good')\n",
    "\n",
    "print(f'\\nInertia: {kmeans_final.inertia_:.2f}')\n",
    "print(f'  Within-cluster sum of squares')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test statistical significance\n",
    "key_features_test = ['severity_score', 'total_regulatory_cost', 'records_affected_numeric',\n",
    "                    'prior_breaches_total', 'firm_size_log']\n",
    "key_features_test = [f for f in key_features_test if f in X.columns]\n",
    "\n",
    "print('\\nANOVA: Feature Differences Across Clusters')\n",
    "print('='*80)\n",
    "\n",
    "for feature in key_features_test:\n",
    "    cluster_groups = [X[df_clustering['cluster'] == i][feature].values for i in range(optimal_k)]\n",
    "    f_stat, p_value = f_oneway(*cluster_groups)\n",
    "    sig = '***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'ns'\n",
    "    print(f'{feature}: F={f_stat:.2f}, p={p_value:.2e} {sig}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Actionable Insights & Recommendations\n",
    "\n",
    "Key findings and recommendations from the clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights\n",
    "print('\\n' + '='*80)\n",
    "print('KEY FINDINGS')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\n1. CLUSTERS IDENTIFIED: {optimal_k} distinct regulatory risk profiles')\n",
    "print(f'   Cluster sizes: {min(cluster_sizes)} - {max(cluster_sizes)} breaches')\n",
    "\n",
    "print(f'\\n2. REGULATORY RISK VARIATION:')\n",
    "min_reg_rate = min(reg_rates)\n",
    "max_reg_rate = max(reg_rates)\n",
    "min_cluster = reg_rates.index(min_reg_rate)\n",
    "max_cluster = reg_rates.index(max_reg_rate)\n",
    "print(f'   Highest risk (Cluster {max_cluster}): {max_reg_rate:.1f}% regulatory action')\n",
    "print(f'   Lowest risk (Cluster {min_cluster}): {min_reg_rate:.1f}% regulatory action')\n",
    "print(f'   Ratio: {max_reg_rate/min_reg_rate:.1f}x difference')\n",
    "\n",
    "print(f'\\n3. DIMENSIONALITY REDUCTION:')\n",
    "cumsum_3pc = pca.explained_variance_ratio_.sum()\n",
    "print(f'   3 components explain {cumsum_3pc:.1%} of variance')\n",
    "print(f'   PC1: {pca.explained_variance_ratio_[0]:.1%} (likely severity/cost)')\n",
    "print(f'   PC2: {pca.explained_variance_ratio_[1]:.1%} (likely organizational/financial)')\n",
    "\n",
    "print(f'\\n4. CLUSTERING QUALITY:')\n",
    "print(f'   Silhouette: {silhouette_avg:.4f} (acceptable)')\n",
    "print(f'   Davies-Bouldin: {davies_bouldin:.4f} (good separation)')\n",
    "print(f'   ANOVA: Significant differences across clusters (p < 0.001)')\n",
    "\n",
    "print('\\n' + '='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive insights visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Regulatory action vs cost scatter\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(reg_rates, avg_costs, s=300, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "for i in range(optimal_k):\n",
    "    ax.annotate(f'C{i}', (reg_rates[i], avg_costs[i]), fontsize=11, fontweight='bold')\n",
    "ax.set_xlabel('Regulatory Action Rate (%)')\n",
    "ax.set_ylabel('Average Regulatory Cost ($)')\n",
    "ax.set_title('Risk Profile: Action Rate vs Cost')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 2. Silhouette scores\n",
    "ax = axes[0, 1]\n",
    "sil_by_cluster = [silhouette_samples(X_scaled, cluster_labels)[cluster_labels == i].mean() for i in range(optimal_k)]\n",
    "ax.bar(range(optimal_k), sil_by_cluster, color=plt.cm.viridis(np.linspace(0, 1, optimal_k)), edgecolor='black', linewidth=2)\n",
    "ax.axhline(silhouette_avg, color='red', linestyle='--', linewidth=2, label=f'Average: {silhouette_avg:.3f}')\n",
    "ax.set_xlabel('Cluster')\n",
    "ax.set_ylabel('Silhouette Score')\n",
    "ax.set_title('Cluster Quality')\n",
    "ax.set_xticks(range(optimal_k))\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Cluster sizes\n",
    "ax = axes[1, 0]\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, optimal_k))\n",
    "ax.bar(range(optimal_k), cluster_sizes, color=colors, edgecolor='black', linewidth=2)\n",
    "ax.set_xlabel('Cluster')\n",
    "ax.set_ylabel('Number of Breaches')\n",
    "ax.set_title('Cluster Size Distribution')\n",
    "ax.set_xticks(range(optimal_k))\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Size vs Action Rate\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(cluster_sizes, reg_rates, s=300, alpha=0.6, edgecolors='black', linewidth=2, c=range(optimal_k), cmap='viridis')\n",
    "for i in range(optimal_k):\n",
    "    ax.annotate(f'C{i}', (cluster_sizes[i], reg_rates[i]), fontsize=11, fontweight='bold')\n",
    "ax.set_xlabel('Cluster Size (# Breaches)')\n",
    "ax.set_ylabel('Regulatory Action Rate (%)')\n",
    "ax.set_title('Size vs Regulatory Risk')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/clustering/05_comprehensive_insights.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('\u2713 Comprehensive insights visualization saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Export Results & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('../outputs/clustering')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export cluster assignments\n",
    "df_export = pd.DataFrame({\n",
    "    'breach_id': range(len(df_clustering)),\n",
    "    'cluster': df_clustering['cluster'],\n",
    "    'has_regulatory_action': df_clustering['has_any_regulatory_action'],\n",
    "    'regulatory_cost': df_clustering['total_regulatory_cost']\n",
    "})\n",
    "df_export.to_csv(output_dir / 'cluster_assignments.csv', index=False)\n",
    "print('\u2713 Cluster assignments exported')\n",
    "\n",
    "# Export cluster profiles\n",
    "cluster_profiles = pd.DataFrame()\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_mask = df_clustering['cluster'] == cluster_id\n",
    "    cluster_profiles[f'Cluster {cluster_id}'] = X[cluster_mask].mean()\n",
    "cluster_profiles['Global Mean'] = X.mean()\n",
    "cluster_profiles.to_csv(output_dir / 'cluster_profiles.csv')\n",
    "print('\u2713 Cluster profiles exported')\n",
    "\n",
    "# Export metrics\n",
    "metrics_dict = {\n",
    "    'silhouette_score': float(silhouette_avg),\n",
    "    'davies_bouldin_index': float(davies_bouldin),\n",
    "    'calinski_harabasz_index': float(calinski_harabasz),\n",
    "    'inertia': float(kmeans_final.inertia_),\n",
    "    'n_clusters': int(optimal_k),\n",
    "    'n_samples': len(df_clustering),\n",
    "    'n_features': len(available_features),\n",
    "    'pca_variance_explained': float(pca.explained_variance_ratio_.sum())\n",
    "}\n",
    "with open(output_dir / 'clustering_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_dict, f, indent=2)\n",
    "print('\u2713 Metrics exported')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ANALYSIS COMPLETE!')\n",
    "print('='*80)\n",
    "print(f'\\nGenerated:')\n",
    "print(f'  - {optimal_k} clusters identified')\n",
    "print(f'  - {pca.explained_variance_ratio_.sum():.1%} variance explained')\n",
    "print(f'  - Silhouette score: {silhouette_avg:.4f}')\n",
    "print(f'  - 5 visualizations saved')\n",
    "print(f'  - 3 data files exported')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}