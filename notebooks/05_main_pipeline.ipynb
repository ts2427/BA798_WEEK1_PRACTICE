{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline: Data Breach Regulatory Action Prediction\n",
    "\n",
    "This notebook orchestrates the complete ML pipeline:\n",
    "1. Data Loading & EDA\n",
    "2. Data Preprocessing\n",
    "3. Model Training\n",
    "4. Model Evaluation\n",
    "5. Results Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Import pipeline modules\n",
    "from config import (\n",
    "    RANDOM_SEED, TARGET_VARIABLE, MODELS_DIR,\n",
    "    METRICS_DIR, FIGURES_DIR\n",
    ")\n",
    "from data_loader import DataLoader\n",
    "from preprocessor import DataPreprocessor\n",
    "from trainer import ModelTrainer\n",
    "from evaluator import ModelEvaluator\n",
    "from utils import setup_output_dirs, get_timestamp\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Setup output directories\n",
    "setup_output_dirs()\n",
    "\n",
    "print(f\"Pipeline initialized. Random seed: {RANDOM_SEED}\")\n",
    "print(f\"Target variable: {TARGET_VARIABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "loader = DataLoader()\n",
    "df = loader.load_dataset()\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes.value_counts()}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum().sort_values(ascending=False).head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate schema\n",
    "loader.validate_schema()\n",
    "\n",
    "# Target distribution\n",
    "print(f\"\\nTarget variable distribution:\")\n",
    "target_dist = loader.get_target_distribution()\n",
    "\n",
    "# Visualize target\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(target_dist.keys(), target_dist.values())\n",
    "plt.xlabel('Regulatory Action')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Target Variable Distribution: {TARGET_VARIABLE}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = loader.split_data()\n",
    "loader.save_splits()\n",
    "\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Fit on training data and transform\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Transform test data\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Save preprocessor\n",
    "preprocessor.save(MODELS_DIR / 'preprocessor.pkl')\n",
    "\n",
    "print(f\"\\nPreprocessed training shape: {X_train_processed.shape}\")\n",
    "print(f\"Preprocessed test shape: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "# Train all models\n",
    "models = trainer.train_all_models(X_train_processed, y_train.values)\n",
    "\n",
    "# Save models\n",
    "for model_name, model in models.items():\n",
    "    filepath = MODELS_DIR / f'{model_name}_model.pkl'\n",
    "    model.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = preprocessor.get_feature_names()\n",
    "\n",
    "# Generate complete evaluation report\n",
    "comparison_df = evaluator.generate_report(\n",
    "    models, X_test_processed, y_test.values, feature_names\n",
    ")\n",
    "\n",
    "print(f\"\\n\\nFinal Model Comparison:\")\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\n‚úì Data loaded: {len(df)} records\")\n",
    "print(f\"‚úì Training set: {len(X_train)} samples\")\n",
    "print(f\"‚úì Test set: {len(X_test)} samples\")\n",
    "print(f\"‚úì Features after preprocessing: {X_train_processed.shape[1]}\")\n",
    "print(f\"‚úì Models trained: 2 (Random Forest, XGBoost)\")\n",
    "\n",
    "# Best model\n",
    "best_model = comparison_df['roc_auc'].idxmax()\n",
    "best_auc = comparison_df['roc_auc'].max()\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model.upper()}\")\n",
    "print(f\"   ROC-AUC: {best_auc:.4f}\")\n",
    "print(f\"   Accuracy: {comparison_df.loc[best_model, 'accuracy']:.4f}\")\n",
    "print(f\"   F1-Score: {comparison_df.loc[best_model, 'f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ Output files saved to:\")\n",
    "print(f\"   Models: {MODELS_DIR}\")\n",
    "print(f\"   Metrics: {METRICS_DIR}\")\n",
    "print(f\"   Figures: {FIGURES_DIR}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Pipeline completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
